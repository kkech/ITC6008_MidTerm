{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkWR9cWlSYWdEbU535nMdo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkech/ITC6008_MidTerm/blob/main/MidTermSearchEngines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "km1R8IanoGAJ",
        "outputId": "1f65df36-312b-4c49-cecf-0f97aded5ce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling who website...\n",
            "Loading bert model...\n",
            "Relevant content found on: https://www.who.int/emergencies/diseases/novel-coronavirus-2019\n",
            "Relevant content found on: https://www.who.int/emergencies/diseases/novel-coronavirus-2019#content\n",
            "Relevant content found on: https://www.who.int/mega-menu/health-topics/popular/mental-disorders\n",
            "Relevant content found on: https://www.who.int/mega-menu/emergencies/emergencies/coronavirus-disease-(covio-19)\n",
            "Relevant content found on: https://www.who.int/mega-menu/emergencies/who-in-emergencies/operations\n",
            "Relevant content found on: https://www.who.int/mega-menu/emergencies/who-in-emergencies/partners\n",
            "Relevant content found on: https://www.who.int/mega-menu/data/dashboards/covid-19-dashboard\n",
            "Relevant content found on: https://www.who.int/mega-menu/data/data-collection/civil-registration-and-vital-statistics\n",
            "Relevant content found on: https://www.who.int/emergencies/diseases/novel-coronavirus-2019/covid-19-policy-briefs\n",
            "Relevant content found on: https://www.who.int/emergencies/diseases/novel-coronavirus-2019/advice-for-public\n",
            "Relevant content found on: https://www.who.int/teams/epi-win/health-workers-and-administrators\n",
            "Relevant content found on: https://www.who.int/emergencies/diseases/novel-coronavirus-2019/question-and-answers-hub\n",
            "Relevant content found on: https://www.who.int/emergencies/diseases/novel-coronavirus-2019/travel-advice\n",
            "Relevant content found on: https://www.who.int/emergencies/diseases/novel-coronavirus-2019/media-resources/science-in-5\n",
            "Relevant content found on: https://www.who.int/news-room/fact-sheets/detail/coronavirus-disease-(covid-19)\n",
            "Relevant content found on: https://www.who.int/emergencies/diseases/novel-coronavirus-2019/technical-guidance\n",
            "Relevant content found on: https://www.who.int/emergencies/diseases/novel-coronavirus-2019/covid-19-vaccines\n",
            "Relevant content found on: https://www.who.int/emergencies/diseases/novel-coronavirus-2019/covid-19-vaccines/advice\n",
            "Relevant content found on: https://www.who.int/emergencies/diseases/novel-coronavirus-2019/situation-reports\n",
            "Relevant content found on: https://www.who.int/emergencies/diseases/novel-coronavirus-2019/donors-and-partners/funding\n",
            "Relevant content found on: https://www.who.int/teams/blueprint/covid-19\n",
            "Relevant content found on: https://www.who.int/emergencies/diseases/novel-coronavirus-2019/technical-guidance/early-investigations\n",
            "Relevant content found on: https://www.who.int/health-topics/coronavirus\n",
            "Relevant content found on: https://www.who.int/activities/tracking-SARS-CoV-2-variants\n",
            "Relevant content found on: https://www.who.int/redirect-pages/page/novel-coronavirus-(covid-19)-situation-dashboard\n",
            "Relevant content found on: https://www.who.int/emergencies/diseases/novel-coronavirus-2019/advice-for-public/videos\n",
            "Relevant content found on: https://www.who.int/emergencies/diseases/novel-coronavirus-2019/training/online-training\n",
            "Relevant content found on: https://www.who.int/emergencies/partners\n",
            "who crawling completed. Total documents: 28\n",
            "Crawling un website...\n",
            "Loading bert model...\n",
            "Relevant content found on: https://www.un.org/en/coronavirus\n",
            "Relevant content found on: https://www.un.org/en/coronavirus#main-content\n",
            "Relevant content found on: https://www.un.org/es/coronavirus\n",
            "Relevant content found on: https://www.un.org/en/coronavirus/un-secretary-general\n",
            "Relevant content found on: https://www.un.org/en/coronavirus/reference-documents-administrators-and-managers\n",
            "Relevant content found on: https://www.un.org/en/coronavirus/vaccination\n",
            "Relevant content found on: https://www.un.org/en/coronavirus/covid-19-faqs\n",
            "Relevant content found on: https://www.un.org/en/coronavirus/un-secretary-general#reports\n",
            "UN crawling completed. Total documents: 8\n",
            "Total documents: 36\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def crawl_and_save(base_url, folder_name, max_depth=1, relevance_threshold=0.7):\n",
        "    visited = set()\n",
        "    document_count = 0\n",
        "\n",
        "    domain = base_url.split(\"//\")[1].split(\"/\")[0]\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "\n",
        "    print(\"Loading bert model...\")\n",
        "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "    covid_context = \"COVID-19 pandemic information, coronavirus symptoms, prevention measures, and outbreak statistics.\"\n",
        "    covid_embedding = model.encode(covid_context, convert_to_tensor=True)\n",
        "\n",
        "    def crawl(url, depth):\n",
        "        nonlocal document_count\n",
        "        if depth > max_depth or url in visited:\n",
        "            return\n",
        "        visited.add(url)\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers)\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                page_text = soup.get_text(separator=' ', strip=True)\n",
        "\n",
        "                sentences = nltk.sent_tokenize(page_text)\n",
        "\n",
        "                relevant_sentences = []\n",
        "                for sentence in sentences:\n",
        "                    sentence_embedding = model.encode(sentence, convert_to_tensor=True)\n",
        "                    similarity = util.pytorch_cos_sim(sentence_embedding, covid_embedding).item()\n",
        "                    if similarity >= relevance_threshold:\n",
        "                        relevant_sentences.append(sentence)\n",
        "\n",
        "                if relevant_sentences:\n",
        "                    print(f\"Relevant content found on: {url}\")  # Print only if relevant\n",
        "                    document_count += 1\n",
        "                    file_name = f\"document_{document_count}.txt\"\n",
        "                    file_path = os.path.join(folder_name, file_name)\n",
        "\n",
        "                    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "                        file.write(f\"url: {url}\\n\\n\")\n",
        "                        file.write(\"\\n\".join(relevant_sentences))\n",
        "\n",
        "                for link in soup.find_all('a', href=True):\n",
        "                    full_url = urljoin(url, link['href'])\n",
        "                    if domain in full_url and re.match(r'^https?:\\/\\/', full_url):\n",
        "                        crawl(full_url, depth + 1)\n",
        "        except Exception as e:\n",
        "            print(f\"Error crawling {url}: {e}\")\n",
        "\n",
        "    os.makedirs(folder_name, exist_ok=True)\n",
        "    crawl(base_url, depth=0)\n",
        "    return document_count\n",
        "\n",
        "# URLs to crawl\n",
        "who_url = \"https://www.who.int/emergencies/diseases/novel-coronavirus-2019\"\n",
        "un_url = \"https://www.un.org/en/coronavirus\"\n",
        "\n",
        "print(\"Crawling who website...\")\n",
        "who_folder = \"WHO_Crawled_Data\"\n",
        "who_document_count = crawl_and_save(who_url, who_folder, max_depth=2, relevance_threshold=0.75)\n",
        "print(f\"who crawling completed. Total documents: {who_document_count}\")\n",
        "\n",
        "print(\"Crawling un website...\")\n",
        "un_folder = \"UN_Crawled_Data\"\n",
        "un_document_count = crawl_and_save(un_url, un_folder, max_depth=2, relevance_threshold=0.75)\n",
        "print(f\"UN crawling completed. Total documents: {un_document_count}\")\n",
        "\n",
        "total_documents = who_document_count + un_document_count\n",
        "print(f\"Total documents: {total_documents}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def extract_text_from_url(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Extract main text content\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "        return text\n",
        "    else:\n",
        "        print(f\"Failed to fetch the url: {url}\")\n",
        "        return None\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    nltk.download('wordnet')\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    filtered_tokens = [\n",
        "        lemmatizer.lemmatize(word)\n",
        "        for word in tokens\n",
        "        if word.isalnum() and word not in stop_words and len(word) > 2\n",
        "    ]\n",
        "\n",
        "    domain_specific_noise = {'read', 'download', 'page', 'section'}\n",
        "    filtered_tokens = [word for word in filtered_tokens if word not in domain_specific_noise]\n",
        "\n",
        "    return ' '.join(filtered_tokens), filtered_tokens\n",
        "\n",
        "def get_top_tfidf_words(text, num_words=20):\n",
        "\n",
        "    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "    tfidf_matrix = vectorizer.fit_transform([text])\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    tfidf_scores = tfidf_matrix.toarray()[0]\n",
        "\n",
        "    word_scores = list(zip(feature_names, tfidf_scores))\n",
        "    sorted_word_scores = sorted(word_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return [word for word, score in sorted_word_scores[:num_words]]\n",
        "\n",
        "# Function to create a vector representation\n",
        "def vectorize_text(tokens, vocabulary):\n",
        "    word_counts = Counter(tokens)\n",
        "    vector = [word_counts.get(word, 0) for word in vocabulary]\n",
        "\n",
        "    return vector\n",
        "\n",
        "# URL to process\n",
        "url = \"https://www.who.int/health-topics/coronavirus#tab=tab_1\"\n",
        "\n",
        "print(\"Extracting text from the url...\")\n",
        "text = extract_text_from_url(url)\n",
        "if text:\n",
        "    print(\"\\nTEXT EXTRACTED:\")\n",
        "    print(text[:1000], \"...\")\n",
        "\n",
        "    print(\"\\nPreprocessing text...\")\n",
        "    cleaned_text, tokens = preprocess_text(text)\n",
        "\n",
        "    # Step 3: Compute TF-IDF and select top words\n",
        "    print(\"\\nSelecting top words using TF-IDF...\")\n",
        "    top_words = get_top_tfidf_words(cleaned_text, num_words=20)\n",
        "    print(f\"Top 20 Words (TF-IDF): {top_words}\")\n",
        "\n",
        "    # Step 4: Create vector representation\n",
        "    print(\"\\nCreating vector representation\")\n",
        "    vector = vectorize_text(tokens, top_words)\n",
        "    print(f\"Vector representation: {vector}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vju1Uvv68dpx",
        "outputId": "8534fe73-52ff-4ada-fd55-0111c7b547f8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting text from the url...\n",
            "\n",
            "TEXT EXTRACTED:\n",
            "Coronavirus Skip to main content Global Regions WHO Regional websites Africa Americas South-East Asia Europe Eastern Mediterranean Western Pacific When autocomplete results are available use up and down arrows to review and enter to select. Select language Select language English العربية 中文 Français Русский Español Donate Donate Home Health Topics All topics A B C D E F G H I J K L M N O P Q R S T U V W X Y Z Resources Fact sheets Facts in pictures Multimedia Podcasts Publications Questions and answers Tools and toolkits Popular Dengue Endometriosis Excessive heat Herpes Mental disorders Mpox Countries All countries A B C D E F G H I J K L M N O P Q R S T U V W X Y Z Regions Africa Americas Europe Eastern Mediterranean South-East Asia Western Pacific WHO in countries Data by country Country presence Country strengthening Country cooperation strategies Newsroom All news News releases Statements Campaigns Events Feature stories Press conferences Speeches Commentaries Photo library Headli ...\n",
            "\n",
            "Preprocessing text...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selecting top words using TF-IDF...\n",
            "Top 20 Words (TF-IDF): ['2024', 'health', 'disease', 'october', 'group', 'coronavirus', 'symptom', 'country', 'emergency', 'epidemiological', 'pandemic', 'report', 'vaccine', 'advisory', 'news', 'september', 'virus', 'global', 'home', 'people']\n",
            "\n",
            "Creating vector representation\n",
            "Vector representation: [28, 21, 13, 12, 11, 9, 8, 7, 7, 7, 7, 7, 7, 6, 6, 6, 6, 5, 5, 5]\n"
          ]
        }
      ]
    }
  ]
}